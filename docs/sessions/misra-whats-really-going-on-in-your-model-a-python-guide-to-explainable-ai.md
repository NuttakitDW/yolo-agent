# What’s Really Going on in Your Model? A Python Guide to Explainable AI

**Speakers:** Yashasvi Misra

**Session Type:** 30-mins talk session

**Level:** Beginner

**Language:** English

**Category:** Machine Learning or AI

## Abstract

We’ve all been there, your machine learning model performs well in testing, but when it comes time to explain why it made a specific prediction, things get murky. In many real-world applications, especially in domains like healthcare, finance, or operations, being able to explain your model isn’t just helpful it’s critical. This talk is a practical walkthrough of explainable AI (XAI) tools in Python, aimed at data scientists and engineers who want to make their models more transparent and trustworthy. We’ll cover libraries like SHAP, LIME, and Captum, and show how to use them to generate both local and global explanations for models ranging from random forests to deep neural nets. You’ll see hands-on examples, common pitfalls to avoid, and ideas for integrating interpretability into your workflow whether you’re trying to debug your model or justify its predictions to a non-technical stakeholder. If you’ve ever wanted to better understand your own models or help others trust them this session is for you.


## About the Speaker(s)

### Yashasvi Misra

Yashasvi Misra is a Data Engineer at Pure Storage and Chair of the NumFOCUS Code of Conduct Working Group, where she helps foster inclusive practices across the open-source ecosystem. She has contributed to foundational projects like NumPy and has been an active part of the Python community since her college days.Yashasvi is also a passionate advocate for diversity and inclusion in tech. She introduced a period leave policy at a previous organization and continues to work toward building more equitable workplaces.She has shared her work and insights at conferences around the world, including PyCon India, PyCon Europe, PyLadiesCon, and PyData Global.