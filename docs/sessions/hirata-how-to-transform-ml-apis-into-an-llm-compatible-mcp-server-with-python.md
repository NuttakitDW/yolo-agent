# How to Transform ML APIs into an LLM-compatible MCP Server with Python

**Speakers:** Tetsuya Hirata

**Session Type:** 30-mins talk session

**Level:** Advanced

**Language:** English

**Category:** Machine Learning or AI

## Abstract

Large language models (LLMs) are no longer just text generators—they're evolving into agents capable of reasoning, deciding, and executing tasks using external tools. But to do that effectively, they require interfaces that are not only callable, but also discoverable and self-describing. REST APIs and OpenAPI-based function calling, while widely used, are still designed for human developers and assume static bindings and schema management. This talk introduces the Model Context Protocol (MCP)—a lightweight, Python-native way to expose machine learning tools in a format that LLM agents can dynamically discover and call. With just a @mcp.tool() decorator, type hints, and docstrings, developers can turn any Python function into an LLM-callable tool without the need for verbose schemas, prompt engineering, or OpenAPI overhead. To illustrate this, I’ll present a focused demo of a speech translation bot that accepts multiple input types (text, audio) and shows how an LLM agent like Claude or a LangChain-based agent can dynamically select and call the appropriate tools—such as speech-to-text and translation—via a unified MCP server. The talk will walk through: - How to break down a modular ML pipeline into callable tools - How to annotate functions with @mcp.tool() and type hints - How LLM agents discover and invoke these tools using list_tools and call_tool - How this approach supports flexible, input-driven workflows beyond static pipelines By the end, developers will see how their existing ML APIs—often built from research code—can be transformed into modular, interoperable, and LLM-compatible systems with minimal effort. Related links: MCP PyPI: <a href="https://pypi.org/project/mcp" target="_blank">https://pypi.org/project/mcp</a> Outline: 1. Introduction: The New Client Is an LLM (3 min) - Many ML APIs originate from research-oriented code that was later wrapped and integrated into applications. - REST APIs as ML APIs were designed for human developers. - In the LLM era, interfaces must be discoverable and callable by language models. 2. Limitations of REST and Function Calling for LLM Use (5 min) - Traditional ML APIs—typically built as REST endpoints—are not designed for language models. They rely on fixed URLs, manual documentation, and human integration. - Function Calling (often using OpenAPI schemas) provides a more structured way to define and invoke tools, but still requires verbose schema definitions and tight, static bindings between functions and tool names. - This makes it difficult to transform existing ML APIs into modular, LLM-friendly components without extra tooling or prompt engineering. - Comparison: MCP vs Function Calling - Ease of definition: Code-first (@mcp.tool) vs Schema-first (OpenAPI YAML/JSON) - Invocation model: list_tools / call_tool vs function_call with hardcoded parameters - LLM compatibility: MCP tools are automatically discoverable and callable by Claude, LangChain, and other LLM agents—no schema parsing or prompt tuning required 3. What is MCP? (4 min) - The Model Context Protocol (MCP) is a lightweight, Python-native way to expose your existing functions as LLM-callable tools. - Instead of writing verbose schemas or wiring up endpoints, you simply decorate a Python function with @mcp.tool() and describe its inputs and behavior using type hints and docstrings. - MCP servers automatically make these tools discoverable (list_tools) and callable (call_tool) by LLM agents like Claude and LangChain—no prompt engineering or OpenAPI required. 4. Focused Demo: STT → Translation (MCP Tools) (10 min) - Showcase a simple pipeline composed of: speech_to_text() (Whisper-based) and translate_to_japanese() (Google Translate or other API) - Demonstrate how each function is transformed into an MCP-compatible tool using @mcp.tool(), with type hints and docstrings. - Present the structure of tools.py and example outputs of list_tools / call_tool via screenshots or logs. - Then, show how an AI agent (such as Claude or a LangChain agent) can dynamically select and call the appropriate tools based on the user's input (e.g., audio vs. text), using the same unified MCP interface. - This demonstrates not just a static pipeline, but how MCP enables flexible, agent-driven workflows that adapt to different input types at runtime. 5. Takeaways & Q&A (5 min) - MCP provides the simplest way to make Python-based ML tools usable by LLMs. - Compared to REST or OpenAPI, MCP enables: Reusability across LLM frameworks - Dynamic discoverability without extra infrastructure - Clean, native Python integration - Answer questions from the audience if time allows. Fallback Plan (If time runs short) - Focus only on speech_to_text() and its conversion into an MCP tool. - Skip translate_to_japanese() and the agent-based workflow explanation. - Emphasize the simplicity of MCP in turning ML functions into LLM-callable tools using Python.


## About the Speaker(s)

### Tetsuya Hirata

Tetsuya is an individual contributor experienced in software engineering within learning science. Formerly at AWS JP, EdTech companies, and UCL IOE's Knowledge Lab. Tetsuya excels in developing prototypes from the early stages of AI/ML-based R&D to productization. https://twitter.com/JesseTetsuya