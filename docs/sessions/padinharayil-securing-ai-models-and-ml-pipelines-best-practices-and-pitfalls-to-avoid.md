# Securing AI Models and ML Pipelines: Best Practices and Pitfalls to Avoid

**Speakers:** Mohammad Saheer Padinharayil

**Session Type:** 30-mins talk session

**Level:** Intermediate

**Language:** English

**Category:** Machine Learning or AI

## Abstract

As machine learning becomes integral to modern applications, the attack surface for AI systems is rapidly expanding. From data poisoning and model inversion to supply chain vulnerabilities in ML pipelines, the risks are real—and often overlooked. In this session, we'll explore key challenges in securing the AI lifecycle. You’ll learn how adversaries exploit weaknesses in model training, deployment, and inference stages—and how to counter them with practical strategies. We’ll walk through: - Threat modeling for ML pipelines - Secure model training and deployment using ML tools - Proven strategies for securing data inputs, model artifacts, and dependencies - CI/CD best practices for AI/ML, including policy enforcement and SBOMs - Real-world case studies of AI system compromises—and what we can learn from them This talk will give you the actionable insight to evaluate and secure your own AI initiatives, ensuring trust and compliance in an era where AI is not just an asset, but a potential liability.


## About the Speaker(s)

### Mohammad Saheer Padinharayil

AI security and cloud-native pipeline expert with about a decade of experience in building scalable, secure ML platforms. Currently leading Payment Integrity and Automation efforts at CVS Health with a focus on ML governance and infrastructure. Frequent speaker on GCP, AI/ML and enterprise-scale MLOps.